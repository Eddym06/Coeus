================================================================================
INFORME TÉCNICO DE INVESTIGACIÓN: PROYECTO COEUS-NANO
================================================================================
FECHA: 19 de Enero de 2026
AUTOR: Equipo de Ingeniería de Investigación & GitHub Copilot
HARDWARE: Intel Core Ultra 9 + NVIDIA RTX 4050 Laptop (6-8GB VRAM)
================================================================================

1. RESUMEN EJECUTIVO
--------------------------------------------------------------------------------
Este informe documenta el diseño, implementación y validación experimental de "Coeus", 
una arquitectura híbrida de redes neuronales diseñada para superar la complejidad 
cuadrática O(N^2) de los Transformers tradicionales.

Actualización V2.5 (18/01/2026): Incorporación de RoPE + GQA.
Actualización V2.6 Logic-Aware (19/01/2026): Entrenamiento "Nightly" Robust (Batch 64 Sim).
- Loss Record: De 10.96 a 0.18 en 1.5 horas.
- Fenómeno "Grokking": Detectado entre step 1000 y 2000.
- Dataset Ampliado: 17.6MB (Mix Lógica/Literatura).

2. ARQUITECTURA TEÓRICA (V2.6 LOGIC-AWARE)
--------------------------------------------------------------------------------
A. RAMA GLOBAL (Neural Memory & Gating):
   - Neural Gating: MLP (Linear->SiLU->Linear) con bias +3.0 para fomentar la retención proactiva.
   - Recurrent Scan: Implementación Log-Space Parallel Scan estable en bfloat16.
   - Dropout 0.1: Activado para evitar memorización destructiva.

B. RAMA LOCAL (Sliding Window):
   - RoPE + GQA (Grouped Query Attention): 6 cabezas Query, 2 KV. 
   - Fix Broadcasting: Corrección de dimensiones [B, H, T, D] en la fusión de GQA.

3. DATASET ESTRATÉGICO "LOGIC MIX LARGE"
--------------------------------------------------------------------------------
Se generó un corpus sintético de ~17.68 MB enfocado en "Drill Learning":
- 40% Literatura (Alice, Frankenstein, Holmes) para estructura gramatical compleja.
- 60% Lógica Sintética (Baja Entropía / Alta Estructura):
    1. Aritmética CoT: Sumas, Restas, Multiplicaciones desglosadas ("Step 1... Therefore").
    2. Silogismos Formales: "All X are Y...".
    3. Pseudocódigo explicado.
- Objetivo: Enseñar al modelo la *forma* del razonamiento deductivo, priorizando la estructura sobre el conocimiento enciclopédico.

4. PROTOCOLO DE ENTRENAMIENTO "ROBUST NIGHTLY"
--------------------------------------------------------------------------------
Configuración diseñada para convergencia profunda en hardware limitado:
- Script: `train_nightly.py`
- Batch Físico: 4 | Accumulation: 16 | Batch Efectivo: 64.
- Optimizador: AdamW (LR 1.6e-4 -> Cosine Decay).
- Reglas de Parada:
    * Patience: 15 iteraciones.
    * Min Burn-In: 2000 pasos obligatorios (Evita paradas prematuras por ruido).
- Resultado Intermedio (Step 2100):
    * Train Loss: 0.2004
    * Val Loss: 0.1843 (Generalización Superior al Entrenamiento - Efecto Dropout).
    * Estado Cognitivo: El modelo ha superado la fase de balbuceo y genera estructuras gramaticales complejas con inserción de lógica.

5. HISTORIAL DE EXPERIMENTOS
--------------------------------------------------------------------------------
[...Fases 1-4 archivadas...]

FASE 5: ENTRENAMIENTO PROLONGADO (V2.6)
- Ejecución: Sesión desatendida de 5000 pasos.
- Hito Step 2100: Val Loss < 0.20. El modelo ha "hackeado" el dataset sintético, aprendiendo los patrones de las plantillas lógicas casi a la perfección.
- La convergencia es extremadamente rápida para un modelo de 120M params, validando la eficiencia de la arquitectura híbrida (Gated Recurrence + Local Attention) para captar dependencias algorítmicas.

6. SIGUIENTES PASOS
--------------------------------------------------------------------------------
1. Finalizar entrenamiento (Step 5000 o Early Stopping).
2. Test de IQ Lógico: Evaluar con `test_logic_iq.py` si la baja Loss se traduce en capacidad aritmética real o solo memorización de plantillas.
3. Git Push: Consolidar la versión `v2.6-logic` en el repositorio remoto.
